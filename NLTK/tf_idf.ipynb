{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The cat sat on the mat.\n",
    "The dog barked at the stranger.\n",
    "The bird is singing in the tree.\n",
    "The sun is shining brightly.\n",
    "The cat and dog are playing together.\n",
    "I love reading books on artificial intelligence.\n",
    "The weather is cold and rainy today.\n",
    "My laptop battery died while working.\n",
    "The football team won the championship.\n",
    "The chef is preparing a delicious meal.\n",
    "She enjoys hiking in the mountains.\n",
    "The train arrived at the station on time.\n",
    "Scientists are researching new medical treatments.\n",
    "The smartphone has a powerful camera.\n",
    "The students are studying for their exams.\n",
    "The movie was full of action and suspense.\n",
    "He listens to music while coding.\n",
    "The artist painted a beautiful landscape.\n",
    "The internet speed is very slow today.\n",
    "The bakery sells fresh bread every morning.\n",
    "'''\n",
    "from io import StringIO\n",
    "text_io = StringIO(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The cat sat on the mat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The dog barked at the stranger.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The bird is singing in the tree.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The sun is shining brightly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The cat and dog are playing together.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love reading books on artificial intelligence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The weather is cold and rainy today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My laptop battery died while working.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The football team won the championship.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The chef is preparing a delicious meal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>She enjoys hiking in the mountains.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The train arrived at the station on time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Scientists are researching new medical treatme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The smartphone has a powerful camera.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The students are studying for their exams.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The movie was full of action and suspense.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>He listens to music while coding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The artist painted a beautiful landscape.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The internet speed is very slow today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The bakery sells fresh bread every morning.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              message\n",
       "0                             The cat sat on the mat.\n",
       "1                     The dog barked at the stranger.\n",
       "2                    The bird is singing in the tree.\n",
       "3                        The sun is shining brightly.\n",
       "4               The cat and dog are playing together.\n",
       "5    I love reading books on artificial intelligence.\n",
       "6                The weather is cold and rainy today.\n",
       "7               My laptop battery died while working.\n",
       "8             The football team won the championship.\n",
       "9             The chef is preparing a delicious meal.\n",
       "10                She enjoys hiking in the mountains.\n",
       "11          The train arrived at the station on time.\n",
       "12  Scientists are researching new medical treatme...\n",
       "13              The smartphone has a powerful camera.\n",
       "14         The students are studying for their exams.\n",
       "15         The movie was full of action and suspense.\n",
       "16                  He listens to music while coding.\n",
       "17          The artist painted a beautiful landscape.\n",
       "18             The internet speed is very slow today.\n",
       "19        The bakery sells fresh bread every morning."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "messages = pd.read_csv(text_io,sep='\\t',names=['message'])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re,nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordlemmatize = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0,len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordlemmatize.lemmatize(word) for word in review if word not in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat sat mat',\n",
       " 'dog barked stranger',\n",
       " 'bird singing tree',\n",
       " 'sun shining brightly',\n",
       " 'cat dog playing together',\n",
       " 'love reading book artificial intelligence',\n",
       " 'weather cold rainy today',\n",
       " 'laptop battery died working',\n",
       " 'football team championship',\n",
       " 'chef preparing delicious meal',\n",
       " 'enjoys hiking mountain',\n",
       " 'train arrived station time',\n",
       " 'scientist researching new medical treatment',\n",
       " 'smartphone powerful camera',\n",
       " 'student studying exam',\n",
       " 'movie full action suspense',\n",
       " 'listens music coding',\n",
       " 'artist painted beautiful landscape',\n",
       " 'internet speed slow today',\n",
       " 'bakery sell fresh bread every morning']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF \n",
    "Term Frequency - Inverse Document Frequency \n",
    "\n",
    "It is a statistical measure used in NLP to evaluate how important a word is in a corpus. It improves upon bag of words by reducing the impact of frequently occuring words (like 'the','is,'and') and giving more importance to rare, meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.528, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0.601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0.601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.528, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.601, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.467, 0, 0, 0, 0, 0, 0, 0.467, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0.531, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.531, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0.447, 0, 0, 0, 0, 0, 0, 0.447, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.447, 0, ..., 0, 0, 0, 0, 0, 0.447, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.515, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0.515, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.453, 0, 0, 0, 0, 0.515, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0.577, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0.5, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0.447, 0, 0.447, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.447, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.577, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0.5, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.515, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.515, 0, 0.515, 0, 0, 0, 0, 0, 0, 0, 0, 0.453, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0.408, 0, 0, 0, 0, 0, 0.408, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.408, 0, 0, 0.408, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.408, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the BOW model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#for Binary BOW enable binary=True\n",
    "tfidf = TfidfVectorizer(max_features=100) \n",
    "x = tfidf.fit_transform(corpus).toarray()\n",
    "x #you can see there are decimal values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30,linewidth=10000,\n",
    "                    formatter=dict(float=lambda x: \"%.3g\" % x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=100,ngram_range=(2,2))\n",
    "x = tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat sat': np.int64(12),\n",
       " 'sat mat': np.int64(40),\n",
       " 'dog barked': np.int64(17),\n",
       " 'barked stranger': np.int64(5),\n",
       " 'bird singing': np.int64(8),\n",
       " 'singing tree': np.int64(44),\n",
       " 'sun shining': np.int64(51),\n",
       " 'shining brightly': np.int64(43),\n",
       " 'cat dog': np.int64(11),\n",
       " 'dog playing': np.int64(18),\n",
       " 'playing together': np.int64(34),\n",
       " 'love reading': np.int64(28),\n",
       " 'reading book': np.int64(38),\n",
       " 'book artificial': np.int64(9),\n",
       " 'artificial intelligence': np.int64(2),\n",
       " 'weather cold': np.int64(54),\n",
       " 'cold rainy': np.int64(14),\n",
       " 'rainy today': np.int64(37),\n",
       " 'laptop battery': np.int64(26),\n",
       " 'battery died': np.int64(6),\n",
       " 'died working': np.int64(16),\n",
       " 'football team': np.int64(21),\n",
       " 'team championship': np.int64(52),\n",
       " 'chef preparing': np.int64(13),\n",
       " 'preparing delicious': np.int64(36),\n",
       " 'delicious meal': np.int64(15),\n",
       " 'enjoys hiking': np.int64(19),\n",
       " 'hiking mountain': np.int64(24),\n",
       " 'train arrived': np.int64(53),\n",
       " 'arrived station': np.int64(1),\n",
       " 'station time': np.int64(48),\n",
       " 'scientist researching': np.int64(41),\n",
       " 'researching new': np.int64(39),\n",
       " 'new medical': np.int64(32),\n",
       " 'medical treatment': np.int64(29),\n",
       " 'smartphone powerful': np.int64(46),\n",
       " 'powerful camera': np.int64(35),\n",
       " 'student studying': np.int64(49),\n",
       " 'studying exam': np.int64(50),\n",
       " 'movie full': np.int64(30),\n",
       " 'full action': np.int64(23),\n",
       " 'action suspense': np.int64(0),\n",
       " 'listens music': np.int64(27),\n",
       " 'music coding': np.int64(31),\n",
       " 'artist painted': np.int64(3),\n",
       " 'painted beautiful': np.int64(33),\n",
       " 'beautiful landscape': np.int64(7),\n",
       " 'internet speed': np.int64(25),\n",
       " 'speed slow': np.int64(47),\n",
       " 'slow today': np.int64(45),\n",
       " 'bakery sell': np.int64(4),\n",
       " 'sell fresh': np.int64(42),\n",
       " 'fresh bread': np.int64(22),\n",
       " 'bread every': np.int64(10),\n",
       " 'every morning': np.int64(20)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
