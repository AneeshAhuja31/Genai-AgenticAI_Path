{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90259e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f8451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm=HuggingFaceEndpoint(repo_id='HuggingFaceH4/zephyr-7b-beta', repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='HuggingFaceH4/zephyr-7b-beta', client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, async_client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, task='text-generation') tokenizer=LlamaTokenizerFast(name_or_path='HuggingFaceH4/zephyr-7b-beta', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<unk>', '<s>', '</s>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ") model_id='HuggingFaceH4/zephyr-7b-beta'\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# First, create the base HuggingFace endpoint\n",
    "base_llm = HuggingFaceEndpoint(\n",
    "    repo_id='HuggingFaceH4/zephyr-7b-beta',\n",
    "    task='text-generation',\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03\n",
    ")\n",
    "\n",
    "# Then use it to create the ChatHuggingFace instance\n",
    "llm = ChatHuggingFace(llm=base_llm)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3454897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"define gradient descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b73b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Gradient descent is an iterative algorithm used in optimization problems to find the local or global minimum of a function by iteratively moving in the direction of steepest descent. In simpler terms, it is a iterative process of making small adjustments to the values of the parameters of a function in order to minimize a cost function or objective function. The algorithm follows this basic process:\\n\\n1. Start with an initial guess or starting point for the parameters of the function.\\n2. Calculate the gradient, which is the slope or steepness of the function at the current point.\\n3. Take a small step in the direction opposite to the gradient, which is also known as the \"descent direction.\"\\n4. Update the current point with the new values found in step 3.\\n5. Repeat steps 2-4 until a predefined stopping criterion is met, such as achieving a minimum error or reaching a maximum number of iterations.\\n\\nGradient descent is often used in machine learning and deep learning algorithms to optimize model parameters during the training process. It is a popular choice due to its simplicity and wide applicability, but can be slow to converge and may get stuck in local minima instead of global minima, making it less reliable for highly complex problems. In such cases, other optimization algorithms like stochastic gradient descent or the Newton-Raphson method may be more appropriate.' additional_kwargs={} response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=293, prompt_tokens=25, total_tokens=318), 'model': '', 'finish_reason': 'stop'} id='run-e2976a5b-561a-4eb5-889c-8c77d048e229-0'\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae1fba",
   "metadata": {},
   "source": [
    "# Chat Prompt Template\n",
    "It is a structured way to define prompts when using chat models. Instead of writing one big prompt, you can build a message-based prompt with roles like \"system\",\"user\",\"assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3a4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9396ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I do not have personal beliefs, but I can provide you with information about langsmith, a uk-based software company that specializes in developing ai technologies focused on natural language processing (nlp). Langsmith's solutions enable intelligent analysis and processing of unstructured text data, such as social media feeds, customer support transcripts, and news articles. Their aim is to help businesses gain insights, automate tasks, and deliver better customer experiences through ai-powered text understanding and generation.\" additional_kwargs={} response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=102, prompt_tokens=59, total_tokens=161), 'model': '', 'finish_reason': 'stop'} id='run-76e8a19c-0a22-4121-baad-dca3db0614a1-0'\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7403a93",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "Output parsers are essential components of the LangChain ecosystem \n",
    "that allow users to interact with and utilize the platform's\n",
    "capabilities by parsing and interpreting the output generated by the        \n",
    "AI models. They play a crucial role in bridging the gap between the AI      \n",
    "models and the external world, enabling users to input, process, and        \n",
    "receive information and data in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a60964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Langsmith is an AI language model developed by the Allen Institute for Artificial Intelligence (AI2). It is trained on a large dataset of text, which enables it to generate human-like responses to various text prompts or questions. Langsmith uses a transformer-based architecture, which is a type of neural network commonly used in natural language processing tasks. Langsmith's training has focused on achieving high accuracy in a variety of NLU (Natural Language Understanding) tasks, such as question answering, sentiment analysis, and text classification. Langsmith's performance is constantly being improved and expanded through ongoing research and development at AI2.\n"
     ]
    }
   ],
   "source": [
    "## stroutput\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser \n",
    "response = chain.invoke({\"input\":\"Can you tell me something about langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a9e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
